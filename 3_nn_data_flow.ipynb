{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data flow through the neural network\n",
    "The objective of this notebook is to illustrate how exactly data passes through a network during training (and operation) that we build out of simple pieces.  During training, the flow of information is separated into a forward pass (predict) and backward pass (adjust).  During operational mode, only the forward pass is used.\n",
    "\n",
    "In addition, we will perform gradient descent on the constructed network.\n",
    "\n",
    "At the same time, we show how to build a network out of Torch modules (shown previously)\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. We build a simple network and manually go through a forward and backward pass [one time only]\n",
    "2. We compare our results to Torch to verify we (and Torch) did the same thing.\n",
    "3. We perform a parameter update again manually and using a Torch function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ready\t\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- let's discuss neural networks and layers in torch\n",
    "require 'nn'\n",
    "Plot = require 'itorch.Plot'\n",
    "print('ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- ensure repeatability\n",
    "torch.manualSeed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- let's build a single layer, which processes inputs via a linear combination and then applies tanh\n",
    "-- which passes the data to layer 2 that does the same thing but with less weights\n",
    "net = nn.Sequential()\n",
    "-- this is 'layer 1'\n",
    "net:add(nn.Linear(10, 5))\n",
    "net:add(nn.Tanh())\n",
    "-- this is 'layer 2'\n",
    "net:add(nn.Linear(5, 1))\n",
    "net:add(nn.Tanh())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nn.Sequential {\n",
       "  [input -> (1) -> (2) -> (3) -> (4) -> output]\n",
       "  (1): nn.Linear(10 -> 5)\n",
       "  (2): nn.Tanh\n",
       "  (3): nn.Linear(5 -> 1)\n",
       "  (4): nn.Tanh\n",
       "}\n",
       "{\n",
       "  gradInput : DoubleTensor - empty\n",
       "  modules : \n",
       "    {\n",
       "      1 : \n",
       "        nn.Linear(10 -> 5)\n",
       "        {\n",
       "          gradBias : DoubleTensor - size: 5\n",
       "          weight : DoubleTensor - size: 5x10\n",
       "          bias : DoubleTensor - size: 5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "          gradInput : DoubleTensor - empty\n",
       "          gradWeight : DoubleTensor - size: 5x10\n",
       "          output : DoubleTensor - empty\n",
       "        }\n",
       "      2 : \n",
       "        nn.Tanh\n",
       "        {\n",
       "          gradInput : DoubleTensor - empty\n",
       "          output : DoubleTensor - empty\n",
       "        }\n",
       "      3 : \n",
       "        nn.Linear(5 -> 1)\n",
       "        {\n",
       "          gradBias : DoubleTensor - size: 1\n",
       "          weight : DoubleTensor - size: 1x5\n",
       "          bias : DoubleTensor - size: 1\n",
       "          gradInput : DoubleTensor - empty\n",
       "          gradWeight : DoubleTensor - size: 1x5\n",
       "          output : DoubleTensor - empty\n",
       "        }\n",
       "      4 : \n",
       "        nn.Tanh\n",
       "        {\n",
       "          gradInput : DoubleTensor - empty\n",
       "          output : DoubleTensor - empty\n",
       "        }"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "    }\n",
       "  output : DoubleTensor - empty\n",
       "}\n",
       "\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- let's print a textual representation of this network\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.1951 -0.0015  0.0772  0.2010 -0.0394  0.0709  0.1805  0.1716  0.1771  0.2281\n",
       "-0.1438 -0.2210 -0.1414 -0.1907  0.1909  0.1993  0.2898 -0.2158  0.2378 -0.2428\n",
       "-0.0899 -0.3081  0.0006 -0.0083  0.1160 -0.1069  0.1345  0.1914 -0.0821 -0.2541\n",
       " 0.0387 -0.2808  0.0019 -0.0363 -0.3075 -0.3022  0.1726 -0.1324  0.2420 -0.1604\n",
       "-0.0855  0.1507  0.0730  0.2462 -0.2686  0.3081 -0.0830 -0.2420  0.2739 -0.0672\n",
       "[torch.DoubleTensor of size 5x10]\n",
       "\n",
       " 0.0957\n",
       "-0.0299\n",
       "-0.0650\n",
       " 0.0241\n",
       " 0.1826\n",
       "[torch.DoubleTensor of size 5]\n",
       "\n",
       " 0.2599 -0.1638 -0.0306  0.0609 -0.0578\n",
       "[torch.DoubleTensor of size 1x5]\n",
       "\n",
       " 0.3302\n",
       "[torch.DoubleTensor of size 1]\n",
       "\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- let's print out the network parameters [they are initially randomized by Torch]\n",
    "\n",
    "-- module 1 weights/bias (part of layer 1)\n",
    "print(net:get(1).weight)\n",
    "print(net:get(1).bias)\n",
    "\n",
    "-- module 3 weights/bias (part of layer 2)\n",
    "print(net:get(3).weight)\n",
    "print(net:get(3).bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing input - the forward pass\n",
    "- we construct a random input\n",
    "- we run it through the network manually and compute the output\n",
    "- we compare the output to the output produced by Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.9701\n",
       "-0.4526\n",
       " 1.7667\n",
       "-0.3455\n",
       " 0.5392\n",
       " 0.1404\n",
       "-1.4901\n",
       "-0.4648\n",
       "-1.5187\n",
       " 0.3638\n",
       "[torch.DoubleTensor of size 10]\n",
       "\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- this is our input to the network\n",
    "x = torch.Tensor(10):normal()\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0.3271\n",
       "[torch.DoubleTensor of size 1]\n",
       "\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- first we propagate the input x through the first layer (modules 1 & 2)\n",
    "h1 = net:get(1).weight * x + net:get(1).bias\n",
    "y2 = torch.tanh(h1)\n",
    "\n",
    "-- now through the second layer (modules 3 & 4)\n",
    "h3 = net:get(3).weight * y2 + net:get(3).bias\n",
    "y4 = torch.tanh(h3)\n",
    "print(y4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0.3271\n",
       "[torch.DoubleTensor of size 1]\n",
       "\n",
       "\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- and this is how we do it in the NN torch toolkit\n",
    "print(net:forward(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward pass - efficient gradient computation\n",
    "- the backward pass, or backpropagation, is an efficient method of computing gradients with respect to all parameters of the network\n",
    "- remember the gradient training for the perceptron? same thing here but more of it :)\n",
    "- let's do this manually just this once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- assume target value is zero for this (arbitrary) instance\n",
    "target = torch.Tensor{0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0.6542\n",
       "[torch.DoubleTensor of size 1]\n",
       "\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- we compute the loss assuming we use the MSE loss (without the 1/2 factor up front) \n",
    "dloss_dy4 = (y4 - target) * 2\n",
    "print(dloss_dy4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0.8930\n",
       "[torch.DoubleTensor of size 1]\n",
       "\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- going from back to front, we compute the derivative of y2 w.r.t. h2\n",
    "dy4_dh3 = - torch.pow(y4,2) + 1\n",
    "print(dy4_dh3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.1115 -0.3237 -0.0257 -0.3531 -0.0378\n",
       "[torch.DoubleTensor of size 1x5]\n",
       "\n",
       " 0.5842\n",
       "[torch.DoubleTensor of size 1]\n",
       "\n",
       " 0.1519\n",
       "-0.0957\n",
       "-0.0179\n",
       " 0.0356\n",
       "-0.0338\n",
       "[torch.DoubleTensor of size 5x1]\n",
       "\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- now we need to do three things in module three (linear layer 5->1, see above)\n",
    "\n",
    "-- common term that we can factor out for efficiency\n",
    "dloss_dh3 = dloss_dy4 * dy4_dh3\n",
    "\n",
    "-- compute gradient with respect to weights in module 3\n",
    "w3_grad = (y2 * dloss_dh3):reshape(1,5)\n",
    "print(w3_grad)\n",
    "\n",
    "-- compute gradient with respect to bias in module 3\n",
    "b3_grad = torch.Tensor{1 * dloss_dh3}\n",
    "print(b3_grad)\n",
    "\n",
    "-- compute gradient with respect to inputs of module 3 which are also output of module 2\n",
    "-- we will send this vector on to the next layer\n",
    "dloss_dy2 = torch.reshape(net:get(3).weight * dloss_dh3, 5, 1)\n",
    "print(dloss_dy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0.9636\n",
       " 0.6930\n",
       " 0.9981\n",
       " 0.6347\n",
       " 0.9958\n",
       "[torch.DoubleTensor of size 5]\n",
       "\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- now the next layer is up (module 2 - tanh)\n",
    "-- this is a scalar function applied in parallel, so derivatives here are elementwise\n",
    "dy2_dh1 = -torch.pow(y2,2) + 1\n",
    "print(dy2_dh1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0.1463\n",
       "-0.0663\n",
       "-0.0178\n",
       " 0.0226\n",
       "-0.0336\n",
       "[torch.DoubleTensor of size 5x1]\n",
       "\n",
       "-0.1420 -0.0662  0.2585 -0.0506  0.0789  0.0206 -0.2180 -0.0680 -0.2222  0.0532\n",
       " 0.0643  0.0300 -0.1172  0.0229 -0.0358 -0.0093  0.0988  0.0308  0.1007 -0.0241\n",
       " 0.0173  0.0081 -0.0315  0.0062 -0.0096 -0.0025  0.0265  0.0083  0.0271 -0.0065\n",
       "-0.0219 -0.0102  0.0399 -0.0078  0.0122  0.0032 -0.0337 -0.0105 -0.0343  0.0082\n",
       " 0.0326  0.0152 -0.0594  0.0116 -0.0181 -0.0047  0.0501  0.0156  0.0511 -0.0122\n",
       "[torch.DoubleTensor of size 5x10]\n",
       "\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       " 0.1463\n",
       "-0.0663\n",
       "-0.0178\n",
       " 0.0226\n",
       "-0.0336\n",
       "[torch.DoubleTensor of size 5x1]\n",
       "\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- and finally module 1, which is a linear layer\n",
    "-- reminder: the input to this layer is x itself\n",
    "dloss_dh1 = torch.cmul(dloss_dy2, dy2_dh1)\n",
    "print(dloss_dh1)\n",
    "w1_grad = dloss_dh1 * x:reshape(1,10)\n",
    "print(w1_grad)\n",
    "b1_grad = dloss_dh1\n",
    "print(b1_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And now let Torch do the work for us\n",
    "We first let torch do the backprop and then verify that the results match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10699431113771\t\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- remember the MSE (mean square error) criterion?\n",
    "-- code here: https://github.com/torch/nn/blob/master/MSECriterion.lua\n",
    "-- C/CUDA code here: https://github.com/torch/nn/blob/master/generic/MSECriterion.c\n",
    "loss = nn.MSECriterion()\n",
    "print(loss:updateOutput(y4, target))\n",
    "\n",
    "-- DO NOT FORGET THIS, backward() accumulates gradient parameters\n",
    "net:zeroGradParameters()\n",
    "net:backward(x, loss:backward(y4, target))\n",
    "\n",
    "-- yes, that's it :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.1115 -0.3237 -0.0257 -0.3531 -0.0378\n",
       "[torch.DoubleTensor of size 1x5]\n",
       "\n",
       "-0.1115 -0.3237 -0.0257 -0.3531 -0.0378\n",
       "[torch.DoubleTensor of size 1x5]\n",
       "\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- let's check the gradient for the weights in the 3rd layer\n",
    "print(w3_grad:reshape(1,5))\n",
    "print(net:get(3).gradWeight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0.5842\n",
       "[torch.DoubleTensor of size 1]\n",
       "\n",
       " 0.5842\n",
       "[torch.DoubleTensor of size 1]\n",
       "\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- let's check the bias\n",
    "print(b3_grad)\n",
    "print(net:get(3).gradBias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0.1463\n",
       "-0.0663\n",
       "-0.0178\n",
       " 0.0226\n",
       "-0.0336\n",
       "[torch.DoubleTensor of size 5x1]\n",
       "\n",
       " 0.1463\n",
       "-0.0663\n",
       "-0.0178\n",
       " 0.0226\n",
       "-0.0336\n",
       "[torch.DoubleTensor of size 5]\n",
       "\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- let's look at the gradient for biases in the first level\n",
    "print(b1_grad)\n",
    "print(net:get(1).gradBias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter updates via gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- now, we can perform the gradient descent training manually, using a learning rate\n",
    "lambda = 0.01\n",
    "netc = net:clone()\n",
    "\n",
    "netc:get(3).weights = netc:get(3).weight - w3_grad * 0.01\n",
    "netc:get(3).bias = netc:get(3).bias - b3_grad * 0.01\n",
    "netc:get(1).weights = netc:get(1).weight - w1_grad * 0.01\n",
    "netc:get(1).bias = netc:get(1).bias - b1_grad * 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- let's do the same in torch on the original network\n",
    "net:updateParameters(lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0.0943\n",
       "-0.0292\n",
       "-0.0648\n",
       " 0.0239\n",
       " 0.1829\n",
       "[torch.DoubleTensor of size 5]\n",
       "\n",
       " 0.0943\n",
       "-0.0292\n",
       "-0.0648\n",
       " 0.0239\n",
       " 0.1829\n",
       "[torch.DoubleTensor of size 5]\n",
       "\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- and compare results, e.g. first layer bias\n",
    "print(netc:get(1).bias)\n",
    "print(net:get(1).bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A catalog of transfer functions\n",
    "- we divide transfer functions into\n",
    "  - intrinsically scalar: Tanh, HardTanh, Sigmoid, ReLU, SoftPlus\n",
    "  - intrinsically vector: SoftMax, LogSoftMax\n",
    "- these can all be used with the Linear module (which provides the input weights to the layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- we will use x as input to the layer\n",
    "xs = torch.range(-50,50) / 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intrinsically scalar transfer functions\n",
    "- [Overview](https://github.com/torch/nn/blob/master/doc/transfer.md)\n",
    "- Tanh = hyperbolic tangent [y = (exp(x) - exp(-x)) / (exp(x) + exp(-x))]\n",
    "- HardTanh - a piecewise linear approximation to Tanh\n",
    "- Sigmoid layer [y = 1 / (1 + exp(-x))]\n",
    "- ReLU layer - Rectified Linear layer [y = max(x, 0)], [ReLU/PReLU results](http://arxiv.org/pdf/1502.01852v1.pdf)\n",
    "- SoftPlus layer - a 'soft' version of ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "[string \"ys1 = nn.Tanh():forward(xs)...\"]:6: attempt to call global 'Plot' (a nil value)\nstack traceback:\n\t[string \"ys1 = nn.Tanh():forward(xs)...\"]:6: in main chunk\n\t[C]: in function 'xpcall'\n\t...lka/Packages/torch/install/share/lua/5.1/itorch/main.lua:179: in function <...lka/Packages/torch/install/share/lua/5.1/itorch/main.lua:143>\n\t...lka/Packages/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t.../Packages/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t.../Packages/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t.../Packages/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t...lka/Packages/torch/install/share/lua/5.1/itorch/main.lua:350: in main chunk\n\t[C]: in function 'require'\n\t[string \"arg={'/Users/vejmelka/.ipython/profile_defaul...\"]:1: in main chunk",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "[string \"ys1 = nn.Tanh():forward(xs)...\"]:6: attempt to call global 'Plot' (a nil value)\nstack traceback:\n\t[string \"ys1 = nn.Tanh():forward(xs)...\"]:6: in main chunk\n\t[C]: in function 'xpcall'\n\t...lka/Packages/torch/install/share/lua/5.1/itorch/main.lua:179: in function <...lka/Packages/torch/install/share/lua/5.1/itorch/main.lua:143>\n\t...lka/Packages/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t.../Packages/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t.../Packages/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t.../Packages/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t...lka/Packages/torch/install/share/lua/5.1/itorch/main.lua:350: in main chunk\n\t[C]: in function 'require'\n\t[string \"arg={'/Users/vejmelka/.ipython/profile_defaul...\"]:1: in main chunk"
     ]
    }
   ],
   "source": [
    "ys1 = nn.Tanh():forward(xs)\n",
    "ys2 = nn.HardTanh():forward(xs)\n",
    "\n",
    "ys3 = nn.Sigmoid():forward(xs)\n",
    "\n",
    "p = Plot():line(xs,ys1,'red','Tanh'):line(xs,ys2,'green','HardTanh'):line(xs,ys3,'blue','Sigmoid')\n",
    "p:title('Tanh/HardTanh/Sigmoid activation functions'):legend(true):draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "[string \"ys4 = nn.ReLU():forward(xs)...\"]:3: attempt to call global 'Plot' (a nil value)\nstack traceback:\n\t[string \"ys4 = nn.ReLU():forward(xs)...\"]:3: in main chunk\n\t[C]: in function 'xpcall'\n\t...lka/Packages/torch/install/share/lua/5.1/itorch/main.lua:179: in function <...lka/Packages/torch/install/share/lua/5.1/itorch/main.lua:143>\n\t...lka/Packages/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t.../Packages/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t.../Packages/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t.../Packages/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t...lka/Packages/torch/install/share/lua/5.1/itorch/main.lua:350: in main chunk\n\t[C]: in function 'require'\n\t[string \"arg={'/Users/vejmelka/.ipython/profile_defaul...\"]:1: in main chunk",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "[string \"ys4 = nn.ReLU():forward(xs)...\"]:3: attempt to call global 'Plot' (a nil value)\nstack traceback:\n\t[string \"ys4 = nn.ReLU():forward(xs)...\"]:3: in main chunk\n\t[C]: in function 'xpcall'\n\t...lka/Packages/torch/install/share/lua/5.1/itorch/main.lua:179: in function <...lka/Packages/torch/install/share/lua/5.1/itorch/main.lua:143>\n\t...lka/Packages/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t.../Packages/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t.../Packages/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t.../Packages/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t...lka/Packages/torch/install/share/lua/5.1/itorch/main.lua:350: in main chunk\n\t[C]: in function 'require'\n\t[string \"arg={'/Users/vejmelka/.ipython/profile_defaul...\"]:1: in main chunk"
     ]
    }
   ],
   "source": [
    "ys4 = nn.ReLU():forward(xs)\n",
    "ys5 = nn.SoftPlus():forward(xs)\n",
    "p = Plot():line(xs,ys4,'red','ReLU'):line(xs,ys5,'blue','SoftPlus'):title('ReLU and SoftPlus transfer functions'):legend(true):draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intrinsically vector transfer functions\n",
    "- these layers are typically used in the last (output layer) of the network\n",
    "- for multiclass classification problems - [SoftMax](https://github.com/torch/nn/blob/master/doc/transfer.md#softmax) layer\n",
    "- logarithmic verions of above: LogSoftMax, LogSigmoid are useful in conjunction with some loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional layers\n",
    "- general info on [convolutional layers](https://github.com/torch/nn/blob/master/doc/convolution.md)\n",
    "- [nn.TemporalConvolution](https://github.com/torch/nn/blob/master/doc/convolution.md#nn.TemporalConvolution) is a 1D convolutional operator suitable for e.g. time series\n",
    "- [nn.SpatialConvolution](https://github.com/torch/nn/blob/master/doc/convolution.md#spatialconvolution) layer is a 2D convolution (for use with images)\n",
    "- max pooling and subsampling layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nn.TemporalConvolution\n",
       "{\n",
       "  gradBias : DoubleTensor - size: 1\n",
       "  dW : 1\n",
       "  bias : DoubleTensor - size: 1\n",
       "  gradInput : DoubleTensor - empty\n",
       "  weight : DoubleTensor - size: 1x2\n",
       "  inputFrameSize : 1\n",
       "  gradWeight : DoubleTensor - size: 1x2\n",
       "  outputFrameSize : 1\n",
       "  kW : 2\n",
       "  output : DoubleTensor - empty\n",
       "}\n",
       "-1  1\n",
       "[torch.DoubleTensor of size 1x2]\n",
       "\n",
       " 0\n",
       "[torch.DoubleTensor of size 1]\n",
       "\n"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = nn.TemporalConvolution(1, 1, 2)\n",
    "print(conv)\n",
    "conv.weight[1][1] = -1\n",
    "conv.weight[1][2] = 1\n",
    "conv.bias[1] = 0\n",
    "print(conv.weight)\n",
    "print(conv.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xs = torch.rand(50,1) * 0.2\n",
    "xs[{{15,30},1}] = xs[{{15,30},1}] + 1\n",
    "ys = conv:forward(xs)\n",
    "noutput = (xs:size(1) - 2) / 1 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- reshape the vectors for plotting\n",
    "time = torch.range(1,noutput)\n",
    "xsp = xs[{{2,noutput+1},{}}]:reshape(noutput)\n",
    "ysp = ys:reshape(noutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "[string \"p = Plot():title('Effect of 1D conf [shifted]...\"]:1: attempt to call global 'Plot' (a nil value)\nstack traceback:\n\t[string \"p = Plot():title('Effect of 1D conf [shifted]...\"]:1: in main chunk\n\t[C]: in function 'xpcall'\n\t...lka/Packages/torch/install/share/lua/5.1/itorch/main.lua:179: in function <...lka/Packages/torch/install/share/lua/5.1/itorch/main.lua:143>\n\t...lka/Packages/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t.../Packages/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t.../Packages/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t.../Packages/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t...lka/Packages/torch/install/share/lua/5.1/itorch/main.lua:350: in main chunk\n\t[C]: in function 'require'\n\t[string \"arg={'/Users/vejmelka/.ipython/profile_defaul...\"]:1: in main chunk",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "[string \"p = Plot():title('Effect of 1D conf [shifted]...\"]:1: attempt to call global 'Plot' (a nil value)\nstack traceback:\n\t[string \"p = Plot():title('Effect of 1D conf [shifted]...\"]:1: in main chunk\n\t[C]: in function 'xpcall'\n\t...lka/Packages/torch/install/share/lua/5.1/itorch/main.lua:179: in function <...lka/Packages/torch/install/share/lua/5.1/itorch/main.lua:143>\n\t...lka/Packages/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t.../Packages/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t.../Packages/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t.../Packages/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t...lka/Packages/torch/install/share/lua/5.1/itorch/main.lua:350: in main chunk\n\t[C]: in function 'require'\n\t[string \"arg={'/Users/vejmelka/.ipython/profile_defaul...\"]:1: in main chunk"
     ]
    }
   ],
   "source": [
    "p = Plot():title('Effect of 1D conf [shifted]'):line(time,xsp+1,'blue','input'):line(time, ysp-1,'red','output')\n",
    "p:xaxis('samples'):yaxis('signal'):legend(true):draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==> loading dataset\t\n"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- snippet shamelessly ripped from https://github.com/torch/tutorials/blob/master/A_datasets/mnist.lua \n",
    "-- load the training data (downloads when first used)\n",
    "tar = 'http://torch7.s3-website-us-east-1.amazonaws.com/data/mnist.t7.tgz'\n",
    "\n",
    "if not paths.dirp('mnist.t7') then\n",
    "   print('==> downloading dataset')\n",
    "   os.execute('wget ' .. tar)\n",
    "   os.execute('tar xvf ' .. paths.basename(tar))\n",
    "end\n",
    "test_file = 'mnist.t7/test_32x32.t7'\n",
    "\n",
    "----------------------------------------------------------------------\n",
    "print('==> loading dataset')\n",
    "\n",
    "-- We load the dataset from disk, it's straightforward\n",
    "test_data = torch.load(test_file,'ascii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAAA4klEQVQ4jWNgGAVEAUYYw9HJwIlx75+1Gz9jV7j769+/f0/+/fv3sQ92BY8PCPPzM/Frdn7/44lVgZYelOH/8T03fldt/muNzGXCULCdUR6Zy4Kh4P///wwMDPy6DDozGBiQvMnAwBAQasv7mYWf4/tvBkYmbobvPKgmMM8J5/jPwMfA+J/tDgPj2+P3tqEZrfimyktOTk5uz19NHM6fZsDAwMDAsOU7Lw4FUHD7IAoXw5usyn/wK3Bj2IVfgQ6DDn4FVxiv4leg8f8GfgVyDA/xK7jAUI/Cx4isFYJX0YVoDQAMSDtVjR3TTAAAAABJRU5ErkJggg==",
      "text/plain": [
       "Console does not support images"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 32,
       "width": 32
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Size of the input is 1 x 32 x 32\t\n"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp2d = test_data.data[{244}]:type('torch.DoubleTensor')\n",
    "print('Size of the input is ' .. inp2d:size(1) .. ' x ' .. inp2d:size(2) .. ' x ' .. inp2d:size(3))\n",
    "itorch.image(inp2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conv2 = nn.SpatialConvolution(1, 1, 3, 3)\n",
    "print(conv2)\n",
    "conv2.weight[{1,1,{1,3},{1,3}}] = -1\n",
    "conv2.weight[{1,1,2,2}] = 8\n",
    "conv2.bias[1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAB4AAAAeCAAAAAAeW/F+AAABdUlEQVQokc3SS0sCURQH8P+dmaszo6Om9jBTrCwyqChrE4VBe79BOxfVyr5GQdv6LNEiiDZBia1CS6g0CybT1PHttJAYH4zr7uZy+HHOveeeSyIYtpih+p+Z62y1H1kuwThid4pkkOXU2Ip7k9zw1zGXmx/g7LHEqY273WZQOX/y8/3cmLs6dPBFke6Ej04yM1p9EgGANyuTczFoFVJCdPIgqKV3bj7BMS7KsobRZTlB7Ep/Y1QUKQBAGI9RZxXtaiGbK2uNAajkst81fCyl4y/OttGvhC/n/84GkHueXl00FCv5luCxENYiLGwFtOzm+5G/qai2dqtpyubZ0kP+1dJdXApFv9IUyf3tPYeJ4c12j6mbC4+quEFR9fo8IUP/m4PznsFFoaqBmEoHRgLYAADVsu/T2jWUvoEW14sXLHS5NqsGeX1uxEmC0+eWX8hI+tx2V5J1fTbLU6fdv4ld62EmWb8vWbSY61GYvLfEAV0mktQT/wKUO3GNG+KPHgAAAABJRU5ErkJggg==",
      "text/plain": [
       "Console does not support images"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 30,
       "width": 30
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "  1\n",
       " 30\n",
       " 30\n",
       "[torch.LongStorage of size 3]\n",
       "\n"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out2d = conv2:forward(inp2d)\n",
    "itorch.image(out2d)\n",
    "print(out2d:size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important loss functions [or criteria]\n",
    "- [Mean Square Error](https://github.com/torch/nn/blob/master/doc/criterion.md#nn.MSECriterion)\n",
    "- [Negative Log Likelihood](https://github.com/torch/nn/blob/master/doc/criterion.md#nn.ClassNLLCriterion) (for use with LogSoftMax layer as it expects logarithms on input\n",
    "- [Cross Entropy](https://github.com/torch/nn/blob/master/doc/criterion.md#crossentropycriterion) combines LogSoftMax with NLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "20100"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
