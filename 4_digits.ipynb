{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First brush with the MNIST digits problem\n",
    "The objective of this notebook is to introduce the MNIST digits classification challenge and implement a first solution that we will keep improving.\n",
    "\n",
    "The dataset was obtained by US Post office from zip codes.  Task: recognize the number from it's image. This is a 10-class problem with a training set of 50k 32x32 images and test set of 10k 32x32 images.  Build a neural network that can correctly identify the images and achieve an error rate of less than 3% [on this 10-class problem!]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "require 'nn'\n",
    "require 'optim'\n",
    "require 'paths'\n",
    "print('ready')\n",
    "\n",
    "-- set the number of threads [set to number of cores]\n",
    "torch.setnumthreads(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- snippet shamelessly ripped from https://github.com/torch/tutorials/blob/master/A_datasets/mnist.lua \n",
    "-- load the training data (downloads when first used)\n",
    "tar = 'http://torch7.s3-website-us-east-1.amazonaws.com/data/mnist.t7.tgz'\n",
    "\n",
    "if not paths.dirp('mnist.t7') then\n",
    "   print('==> downloading dataset')\n",
    "   os.execute('wget ' .. tar)\n",
    "   os.execute('tar xvf ' .. paths.basename(tar))\n",
    "end\n",
    "\n",
    "train_file = 'mnist.t7/train_32x32.t7'\n",
    "test_file = 'mnist.t7/test_32x32.t7'\n",
    "\n",
    "----------------------------------------------------------------------\n",
    "print('==> loading dataset')\n",
    "\n",
    "-- We load the dataset from disk, it's straightforward\n",
    "\n",
    "train_data = torch.load(train_file,'ascii')\n",
    "test_data = torch.load(test_file,'ascii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- print some training data\n",
    "print(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- example image\n",
    "itorch.image(train_data.data[{500,1}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- let's preprocess the data a bit, so they are between 0 and 1\n",
    "train_data.data = train_data.data:reshape(60000, 32*32):type('torch.DoubleTensor'):mul(1.0 / 256.0)\n",
    "test_data.data = test_data.data:reshape(10000, 32*32):type('torch.DoubleTensor'):mul(1.0 / 256.0)\n",
    "\n",
    "-- remove mean\n",
    "--train_data.data = train_data.data - torch.repeatTensor(torch.mean(train_data.data,2),1,1024)\n",
    "--test_data.data = test_data.data - torch.repeatTensor(torch.mean(test_data.data,2),1,1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- build our NN model\n",
    "net = nn.Sequential()\n",
    "net:add(nn.Linear(32*32, 250))\n",
    "net:add(nn.SoftSign())\n",
    "net:add(nn.Linear(250,10))\n",
    "loss = nn.CrossEntropyCriterion()\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization code\n",
    "- here, the network is initialized to small random values\n",
    "- good initialization is critical, e.g. recently [Sutskever, 2013](http://www.cs.toronto.edu/~fritz/absps/momentum.pdf)\n",
    "- we use initialization due to [Glorot & Bengio, 2010](http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf)\n",
    "- when you modify the network, add code to randomly initialize also the new nn.Linear layers!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- Glorot & Bengio per-layer initialization of network (we will discuss this topic more tomorrow)\n",
    "local tmp = math.sqrt(1. / 1024)\n",
    "net:get(1).weight:uniform(-tmp, tmp)\n",
    "net:get(1).bias:zero() -- per Bengio & Bergstra 2012\n",
    "tmp = math.sqrt(1. / 250)\n",
    "net:get(3).weight:uniform(-tmp, tmp)\n",
    "net:get(3).bias:zero()\n",
    "\n",
    "--parameters:uniform(-0.01, 0.01)\n",
    "print('Random initialization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training code\n",
    "- networks are typically trained in mini-batches, which are a middle ground between fully online and full batch learning\n",
    "- the size of the mini-batch and the learning rate must be adjusted to 'good values', what are those? Unfortunately those are problem specific.\n",
    "- Here's a starting point: learning rate 0.1 and batch size of 128\n",
    "- training proceeds in epochs, one epoch is a full pass through all the training data\n",
    "- we split the epoch into mini-batches of size 128, which the network processes simultaneously\n",
    "- when you change the parameters to see if the network learns well or not, don't forget to rerun the random initialization\n",
    "\n",
    "NOTE: On this [blog post](http://blog.bigml.com/2013/07/01/you-dont-need-coursera-to-get-started-with-machine-learning/), there is a 1-click solution by bigml.com who explain that to get good results (their test error: 90.6% on a subset of 4000 training data and 1000 testing data), you don't need Coursera.  Maybe true but we're still going to at least match that score in this first example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "-- params\n",
    "nepochs = 40\n",
    "lambda = 0.01\n",
    "lambda_decay = 1e-3\n",
    "batch_size = 100\n",
    "etest = 1\n",
    "print_diagnostics = false\n",
    "\n",
    "-- dataset sizes\n",
    "ntrain = train_data.labels:size(1)\n",
    "--ntrain = 10000 -- small subset of the dataset to tune the learning rate\n",
    "--ntrain = 4000\n",
    "ntest = test_data.labels:size(1)\n",
    "--ntest = 1000\n",
    "\n",
    "-- For each epochs\n",
    "for e=1,nepochs do\n",
    "\n",
    "    local timer = torch.Timer() \n",
    "    local err = 0\n",
    "    local misses = 0\n",
    "    local confmat = optim.ConfusionMatrix(10, {'0', '1', '2', '3', '4', '5', '6', '7', '8', '9'})\n",
    "    for b=1,ntrain/batch_size do\n",
    "\n",
    "        -- DO NOT FORGET TO ZERO THE GRADIENT STORAGE\n",
    "        -- BEFORE EACH MINIBATCH\n",
    "        net:zeroGradParameters()\n",
    "\n",
    "        for i=1,batch_size do\n",
    "            -- select current training example\n",
    "            local ndx = (b-1) * batch_size + i\n",
    "            local x = train_data.data[ndx]\n",
    "            local t = train_data.labels[ndx]\n",
    "\n",
    "            -- run it through the network & accumulate error\n",
    "            local y = net:forward(x)\n",
    "            err = err + loss:forward(y, t)\n",
    "            \n",
    "            confmat:add(y,t)\n",
    "            \n",
    "            local _, digit = torch.max(y,1)\n",
    "            if digit[1] ~= t then\n",
    "                misses = misses + 1\n",
    "            end\n",
    "\n",
    "            -- run it backward from the loss criterion\n",
    "            local dt_dy = loss:backward(y, t) \n",
    "            net:backward(x, dt_dy)\n",
    "        end\n",
    "\n",
    "        -- simple network update (gradient descent with learning rate = lambda)\n",
    "        net:updateParameters(lambda)\n",
    "        \n",
    "    end\n",
    "\n",
    "    print('Training error after EPOCH ' .. e .. ' = ' .. err .. ' misses = ' .. misses .. ' learning_rate = ' .. lambda)\n",
    "    \n",
    "    --FIXME: uncomment if you want to see how the network is learning\n",
    "    --print(confmat:__tostring__())\n",
    "\n",
    "    -- compute testing error every etest epochs\n",
    "    if e % etest == 0 then\n",
    "        local terr = 0\n",
    "        local p = 0\n",
    "        confmat:zero()\n",
    "        for i=1,ntest do\n",
    "            local x = test_data.data[i]\n",
    "            local t = test_data.labels[i]\n",
    "            local y = net:forward(x)\n",
    "            local logprob, digit = torch.max(y,1)\n",
    "            if digit[1] ~= t then \n",
    "                --print('test[' .. i .. '] tgt ' .. (t-1) .. ' prediction ' .. (digit-1))\n",
    "                p = p + 1\n",
    "            end\n",
    "            confmat:add(y,t)\n",
    "            terr = terr + loss:forward(y, t)\n",
    "        end\n",
    "        print('Testing error after epoch ' .. e .. ' = ' .. terr .. ' in counts ' .. p .. '/' .. ntest)\n",
    "        if e == nepochs then\n",
    "            print('**** FINAL CONFUSION MATRIX ****')\n",
    "            print(confmat:__tostring__())\n",
    "        end\n",
    "    end\n",
    "\n",
    "    -- print diagnostic info about the training process\n",
    "    if print_diagnostics then\n",
    "        print('       DIAGNOSTICS       ')\n",
    "        for _, l in ipairs({1,3}) do\n",
    "            local w = net:get(l).weight\n",
    "            local nw = w:nElement()\n",
    "            local w_max =  torch.max(torch.abs(w))\n",
    "            local wg = net:get(l).gradWeight\n",
    "            local wg_max =  torch.max(torch.abs(wg))\n",
    "            local b = net:get(l).bias\n",
    "            local nb = b:nElement()\n",
    "            local b_max = torch.max(torch.abs(b))\n",
    "            print('layer ' .. l)\n",
    "            print('  weight norms max/2/1 [' .. w_max .. ', ' .. torch.norm(w,2)/nw .. ', ' .. torch.norm(w,1)/nw .. ']')\n",
    "            print('  w/grad norms max/2/1 [' .. wg_max .. ', ' .. torch.norm(wg,2)/nw .. ', ' .. torch.norm(wg,1)/nw .. ']')\n",
    "            print('  bias norms max/2/1 [' .. b_max .. ', ' .. torch.norm(b,2)/nb .. ', ' .. torch.norm(b,1)/nb .. ']')\n",
    "        end\n",
    "    end\n",
    "\n",
    "    print('Epoch took ' .. timer:time().real .. ' seconds.')\n",
    "    \n",
    "    -- apply decay before next epoch\n",
    "    lambda = lambda * (1 - lambda_decay)\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- let's plot the weights\n",
    "gnuplot.imagesc(net:get(1).weight, 'color')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suggested experiments\n",
    "Summarize and record your observations for each of these problems.\n",
    "- change the learning rate, observe the magnitude of the gradient updates\n",
    "- change the number of neurons, observe behavior\n",
    "- switch on the diagnostic code and observe the gradient norms in detail\n",
    "- add another layer to the network (e.g. a 32 neuron layer after layer 1, Tanh again) - don't forget to add initialization !\n",
    "- try to change the activation functions to nn.Sigmoid(), is training easier?\n",
    "- instead of using the same learning rate for all layers, try out custom learning rates! See the TODO part in the code. Where would you increase them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- this always computes the error over the entire test set\n",
    "local terr = 0\n",
    "local p = 0\n",
    "local ntest = test_data.labels:size(1)\n",
    "errlist = {}\n",
    "for i=1,ntest do\n",
    "    local x = test_data.data[i]\n",
    "    local t = test_data.labels[i]\n",
    "    local y = net:forward(x)\n",
    "    local _, digit = torch.max(y,1)\n",
    "    digit = digit[1]\n",
    "    if digit ~= t then \n",
    "        errlist[#errlist+1]={i, (t-1), (digit-1)}\n",
    "        p = p + 1\n",
    "    end\n",
    "    terr = terr + loss:forward(y, t)\n",
    "end\n",
    "--print(errlist)\n",
    "print('Testing error = ' .. terr .. ' in counts ' .. p .. '/' .. ntest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "-- This can be used to examine the networks performance on any chosen test digit.\n",
    "ndx = 126\n",
    "itorch.image(test_data.data[ndx]:reshape(32,32))\n",
    "y = net:forward(test_data.data[ndx])\n",
    "print(torch.cat(torch.range(0,9),y,2))\n",
    "logprob, digit = torch.max(y,1)\n",
    "digit = digit[1] - 1\n",
    "target = test_data.labels[ndx]-1\n",
    "print('Prediction ' .. digit .. ' real label ' .. target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- test on an arbitrary image\n",
    "img = image.load('handwritten.png')\n",
    "img = img[1]\n",
    "itorch.image(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = nn.SoftMax():forward(net:forward(img:reshape(1024)))\n",
    "print(torch.cat(torch.range(0,9),y,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "20100"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
